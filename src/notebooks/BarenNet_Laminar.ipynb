{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-06 13:53:48.428500: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-06 13:53:48.428515: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "  sys.path.append(module_path)\n",
    "\n",
    "from utils.utils import Create_Similarity_Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we will create our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 1)            0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " lambda_1 (Lambda)              (None, 1)            0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " similarity_layer_1 (Dense)     (None, 1)            1           ['lambda[0][0]']                 \n",
      "                                                                                                  \n",
      " lambda_2 (Lambda)              (None, 1)            0           ['lambda_1[0][0]']               \n",
      "                                                                                                  \n",
      " multiply (Multiply)            (None, 1)            0           ['similarity_layer_1[0][0]',     \n",
      "                                                                  'lambda_2[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 1)            0           ['multiply[0][0]']               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          256         ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 64)           8256        ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 32)           2080        ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1)            33          ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " multiplication_layer (Dense)   (None, 1)            1           ['lambda[0][0]']                 \n",
      "                                                                                                  \n",
      " multiply_1 (Multiply)          (None, 1)            0           ['dense_3[0][0]',                \n",
      "                                                                  'multiplication_layer[0][0]']   \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 10,627\n",
      "Trainable params: 10,627\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-06 13:53:51.163550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-06 13:53:51.163933: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-06 13:53:51.163973: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-07-06 13:53:51.164009: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-07-06 13:53:51.164044: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-07-06 13:53:51.164079: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-07-06 13:53:51.164113: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-07-06 13:53:51.164148: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-07-06 13:53:51.164182: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-07-06 13:53:51.164188: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-07-06 13:53:51.164384: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "n_nonsimilar = 1\n",
    "n_similar = 1\n",
    "\n",
    "### We will look for similarity in the last n_similar parameters of the inputs.\n",
    "\n",
    "model = Create_Similarity_Model(n_nonsimilar = n_nonsimilar, n_similar = n_similar)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss, optimizer and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 'mean_squared_error'\n",
    "optimizer = 'adam'\n",
    "metrics = ['mean_squared_error']\n",
    "\n",
    "model.compile(loss = loss, optimizer = optimizer, metrics = metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fabricate our data simulating the laminar flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_re_tau = np.linspace(start = 1, stop = 100, num = 100)\n",
    "Re_tau = []\n",
    "Y_ =[]\n",
    "U_ = []\n",
    "dic_x = {}\n",
    "dic_y = {}\n",
    "\n",
    "for re_tau in possible_re_tau:\n",
    "    possible_y_= np.linspace(start = 0.1, stop = re_tau, num = 100)\n",
    "    for y_ in possible_y_:\n",
    "        Y_.append(y_)\n",
    "        Re_tau.append(re_tau)\n",
    "        u_ = y_ - ((1 / (2*re_tau)) * ((y_) ** 2))\n",
    "        U_.append(u_)\n",
    "\n",
    "Y_ = np.log(Y_)\n",
    "Re_tau = np.log(Re_tau)\n",
    "\n",
    "dic_x['y+'] = Y_\n",
    "dic_x['re_tau'] = Re_tau\n",
    "dic_y['u+'] = U_\n",
    "\n",
    "nonsimilar_keys = ['y+']\n",
    "\n",
    "xtrain = pd.DataFrame.from_dict(dic_x).values\n",
    "ytrain = pd.DataFrame.from_dict(dic_y).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "313/313 [==============================] - 1s 878us/step - loss: 358.6801 - mean_squared_error: 358.6801\n",
      "Epoch 2/1000\n",
      "313/313 [==============================] - 0s 903us/step - loss: 190.8252 - mean_squared_error: 190.8252\n",
      "Epoch 3/1000\n",
      "313/313 [==============================] - 0s 903us/step - loss: 144.7307 - mean_squared_error: 144.7307\n",
      "Epoch 4/1000\n",
      "313/313 [==============================] - 0s 866us/step - loss: 126.5067 - mean_squared_error: 126.5067\n",
      "Epoch 5/1000\n",
      "313/313 [==============================] - 0s 867us/step - loss: 116.0855 - mean_squared_error: 116.0855\n",
      "Epoch 6/1000\n",
      "313/313 [==============================] - 0s 871us/step - loss: 108.0653 - mean_squared_error: 108.0653\n",
      "Epoch 7/1000\n",
      "313/313 [==============================] - 0s 864us/step - loss: 101.7079 - mean_squared_error: 101.7079\n",
      "Epoch 8/1000\n",
      "313/313 [==============================] - 0s 870us/step - loss: 71.9714 - mean_squared_error: 71.9714\n",
      "Epoch 9/1000\n",
      "313/313 [==============================] - 0s 906us/step - loss: 29.4608 - mean_squared_error: 29.4608\n",
      "Epoch 10/1000\n",
      "313/313 [==============================] - 0s 907us/step - loss: 24.1450 - mean_squared_error: 24.1450\n",
      "Epoch 11/1000\n",
      "313/313 [==============================] - 0s 925us/step - loss: 19.6670 - mean_squared_error: 19.6670\n",
      "Epoch 12/1000\n",
      "313/313 [==============================] - 0s 865us/step - loss: 15.8970 - mean_squared_error: 15.8970\n",
      "Epoch 13/1000\n",
      "313/313 [==============================] - 0s 913us/step - loss: 13.2720 - mean_squared_error: 13.2720\n",
      "Epoch 14/1000\n",
      "313/313 [==============================] - 0s 894us/step - loss: 10.4790 - mean_squared_error: 10.4790\n",
      "Epoch 15/1000\n",
      "313/313 [==============================] - 0s 870us/step - loss: 8.5886 - mean_squared_error: 8.5886\n",
      "Epoch 16/1000\n",
      "313/313 [==============================] - 0s 869us/step - loss: 7.8659 - mean_squared_error: 7.8659\n",
      "Epoch 17/1000\n",
      "313/313 [==============================] - 0s 860us/step - loss: 5.8628 - mean_squared_error: 5.8628\n",
      "Epoch 18/1000\n",
      "313/313 [==============================] - 0s 861us/step - loss: 5.1932 - mean_squared_error: 5.1932\n",
      "Epoch 19/1000\n",
      "313/313 [==============================] - 0s 858us/step - loss: 4.2776 - mean_squared_error: 4.2776\n",
      "Epoch 20/1000\n",
      "313/313 [==============================] - 0s 890us/step - loss: 3.1467 - mean_squared_error: 3.1467\n",
      "Epoch 21/1000\n",
      "313/313 [==============================] - 0s 913us/step - loss: 2.5794 - mean_squared_error: 2.5794\n",
      "Epoch 22/1000\n",
      "313/313 [==============================] - 0s 892us/step - loss: 2.3741 - mean_squared_error: 2.3741\n",
      "Epoch 23/1000\n",
      "313/313 [==============================] - 0s 841us/step - loss: 1.9564 - mean_squared_error: 1.9564\n",
      "Epoch 24/1000\n",
      "313/313 [==============================] - 0s 870us/step - loss: 1.5361 - mean_squared_error: 1.5361\n",
      "Epoch 25/1000\n",
      "313/313 [==============================] - 0s 861us/step - loss: 1.1365 - mean_squared_error: 1.1365\n",
      "Epoch 26/1000\n",
      "313/313 [==============================] - 0s 859us/step - loss: 1.2363 - mean_squared_error: 1.2363\n",
      "Epoch 27/1000\n",
      "313/313 [==============================] - 0s 890us/step - loss: 1.5073 - mean_squared_error: 1.5073\n",
      "Epoch 28/1000\n",
      "313/313 [==============================] - 0s 895us/step - loss: 0.8874 - mean_squared_error: 0.8874\n",
      "Epoch 29/1000\n",
      "313/313 [==============================] - 0s 858us/step - loss: 1.0859 - mean_squared_error: 1.0859\n",
      "Epoch 30/1000\n",
      "313/313 [==============================] - 0s 876us/step - loss: 0.8266 - mean_squared_error: 0.8266\n",
      "Epoch 31/1000\n",
      "313/313 [==============================] - 0s 953us/step - loss: 0.6270 - mean_squared_error: 0.6270\n",
      "Epoch 32/1000\n",
      "313/313 [==============================] - 0s 874us/step - loss: 0.7611 - mean_squared_error: 0.7611\n",
      "Epoch 33/1000\n",
      "313/313 [==============================] - 0s 891us/step - loss: 1.1350 - mean_squared_error: 1.1350\n",
      "Epoch 34/1000\n",
      "313/313 [==============================] - 0s 854us/step - loss: 0.5025 - mean_squared_error: 0.5025\n",
      "Epoch 35/1000\n",
      "313/313 [==============================] - 0s 873us/step - loss: 0.5678 - mean_squared_error: 0.5678\n",
      "Epoch 36/1000\n",
      "313/313 [==============================] - 0s 910us/step - loss: 0.8061 - mean_squared_error: 0.8061\n",
      "Epoch 37/1000\n",
      "313/313 [==============================] - 0s 887us/step - loss: 0.3939 - mean_squared_error: 0.3939\n",
      "Epoch 38/1000\n",
      "313/313 [==============================] - 0s 885us/step - loss: 0.6982 - mean_squared_error: 0.6982\n",
      "Epoch 39/1000\n",
      "313/313 [==============================] - 0s 903us/step - loss: 0.7181 - mean_squared_error: 0.7181\n",
      "Epoch 40/1000\n",
      "313/313 [==============================] - 0s 887us/step - loss: 0.3842 - mean_squared_error: 0.3842\n",
      "Epoch 41/1000\n",
      "313/313 [==============================] - 0s 868us/step - loss: 0.5845 - mean_squared_error: 0.5845\n",
      "Epoch 42/1000\n",
      "313/313 [==============================] - 0s 879us/step - loss: 0.2835 - mean_squared_error: 0.2835\n",
      "Epoch 43/1000\n",
      "313/313 [==============================] - 0s 888us/step - loss: 1.1314 - mean_squared_error: 1.1314\n",
      "Epoch 44/1000\n",
      "313/313 [==============================] - 0s 863us/step - loss: 0.2225 - mean_squared_error: 0.2225\n",
      "Epoch 45/1000\n",
      "313/313 [==============================] - 0s 911us/step - loss: 0.4457 - mean_squared_error: 0.4457\n",
      "Epoch 46/1000\n",
      "313/313 [==============================] - 0s 867us/step - loss: 1.0190 - mean_squared_error: 1.0190\n",
      "Epoch 47/1000\n",
      "313/313 [==============================] - 0s 918us/step - loss: 0.2609 - mean_squared_error: 0.2609\n",
      "Epoch 48/1000\n",
      "313/313 [==============================] - 0s 882us/step - loss: 0.3679 - mean_squared_error: 0.3679\n",
      "Epoch 49/1000\n",
      "313/313 [==============================] - 0s 870us/step - loss: 0.2844 - mean_squared_error: 0.2844\n",
      "Epoch 50/1000\n",
      "313/313 [==============================] - 0s 865us/step - loss: 1.2797 - mean_squared_error: 1.2797\n",
      "Epoch 51/1000\n",
      "313/313 [==============================] - 0s 866us/step - loss: 0.1694 - mean_squared_error: 0.1694\n",
      "Epoch 52/1000\n",
      "313/313 [==============================] - 0s 877us/step - loss: 0.1472 - mean_squared_error: 0.1472\n",
      "Epoch 53/1000\n",
      "313/313 [==============================] - 0s 890us/step - loss: 0.3291 - mean_squared_error: 0.3291\n",
      "Epoch 54/1000\n",
      "313/313 [==============================] - 0s 889us/step - loss: 0.1121 - mean_squared_error: 0.1121\n",
      "Epoch 55/1000\n",
      "313/313 [==============================] - 0s 881us/step - loss: 0.5100 - mean_squared_error: 0.5100\n",
      "Epoch 56/1000\n",
      "313/313 [==============================] - 0s 891us/step - loss: 0.0879 - mean_squared_error: 0.0879\n",
      "Epoch 57/1000\n",
      "313/313 [==============================] - 0s 934us/step - loss: 0.1526 - mean_squared_error: 0.1526\n",
      "Epoch 58/1000\n",
      "313/313 [==============================] - 0s 871us/step - loss: 0.0879 - mean_squared_error: 0.0879\n",
      "Epoch 59/1000\n",
      "313/313 [==============================] - 0s 868us/step - loss: 0.3665 - mean_squared_error: 0.3665\n",
      "Epoch 60/1000\n",
      "313/313 [==============================] - 0s 861us/step - loss: 0.0616 - mean_squared_error: 0.0616\n",
      "Epoch 61/1000\n",
      "313/313 [==============================] - 0s 941us/step - loss: 0.5267 - mean_squared_error: 0.5267\n",
      "Epoch 62/1000\n",
      "313/313 [==============================] - 0s 843us/step - loss: 0.1108 - mean_squared_error: 0.1108\n",
      "Epoch 63/1000\n",
      "313/313 [==============================] - 0s 851us/step - loss: 0.2248 - mean_squared_error: 0.2248\n",
      "Epoch 64/1000\n",
      "313/313 [==============================] - 0s 887us/step - loss: 0.0898 - mean_squared_error: 0.0898\n",
      "Epoch 65/1000\n",
      "313/313 [==============================] - 0s 872us/step - loss: 0.1439 - mean_squared_error: 0.1439\n",
      "Epoch 66/1000\n",
      "313/313 [==============================] - 0s 883us/step - loss: 0.4168 - mean_squared_error: 0.4168\n",
      "Epoch 67/1000\n",
      "313/313 [==============================] - 0s 933us/step - loss: 0.0427 - mean_squared_error: 0.0427\n",
      "Epoch 68/1000\n",
      "313/313 [==============================] - 0s 899us/step - loss: 0.0413 - mean_squared_error: 0.0413\n",
      "Epoch 69/1000\n",
      "313/313 [==============================] - 0s 885us/step - loss: 0.0774 - mean_squared_error: 0.0774\n",
      "Epoch 70/1000\n",
      "313/313 [==============================] - 0s 868us/step - loss: 0.0382 - mean_squared_error: 0.0382\n",
      "Epoch 71/1000\n",
      "313/313 [==============================] - 0s 866us/step - loss: 0.2384 - mean_squared_error: 0.2384\n",
      "Epoch 72/1000\n",
      "313/313 [==============================] - 0s 874us/step - loss: 0.0215 - mean_squared_error: 0.0215\n",
      "Epoch 73/1000\n",
      "313/313 [==============================] - 0s 853us/step - loss: 0.0494 - mean_squared_error: 0.0494\n",
      "Epoch 74/1000\n",
      "313/313 [==============================] - 0s 876us/step - loss: 0.3975 - mean_squared_error: 0.3975\n",
      "Epoch 75/1000\n",
      "313/313 [==============================] - 0s 869us/step - loss: 0.0204 - mean_squared_error: 0.0204\n",
      "Epoch 76/1000\n",
      "313/313 [==============================] - 0s 869us/step - loss: 0.0234 - mean_squared_error: 0.0234\n",
      "Epoch 77/1000\n",
      "313/313 [==============================] - 0s 869us/step - loss: 0.1173 - mean_squared_error: 0.1173\n",
      "Epoch 78/1000\n",
      "313/313 [==============================] - 0s 905us/step - loss: 0.0288 - mean_squared_error: 0.0288\n",
      "Epoch 79/1000\n",
      "313/313 [==============================] - 0s 897us/step - loss: 0.0962 - mean_squared_error: 0.0962\n",
      "Epoch 80/1000\n",
      "313/313 [==============================] - 0s 875us/step - loss: 0.2010 - mean_squared_error: 0.2010\n",
      "Epoch 81/1000\n",
      "313/313 [==============================] - 0s 894us/step - loss: 0.0290 - mean_squared_error: 0.0290\n",
      "Epoch 82/1000\n",
      "313/313 [==============================] - 0s 870us/step - loss: 0.4819 - mean_squared_error: 0.4819\n",
      "Epoch 83/1000\n",
      "313/313 [==============================] - 0s 865us/step - loss: 0.0232 - mean_squared_error: 0.0232\n",
      "Epoch 84/1000\n",
      "313/313 [==============================] - 0s 870us/step - loss: 0.0081 - mean_squared_error: 0.0081\n",
      "Epoch 85/1000\n",
      "313/313 [==============================] - 0s 896us/step - loss: 0.0129 - mean_squared_error: 0.0129\n",
      "Epoch 86/1000\n",
      "313/313 [==============================] - 0s 869us/step - loss: 0.0387 - mean_squared_error: 0.0387\n",
      "Epoch 87/1000\n",
      "313/313 [==============================] - 0s 890us/step - loss: 0.0317 - mean_squared_error: 0.0317\n",
      "Epoch 88/1000\n",
      "313/313 [==============================] - 0s 889us/step - loss: 0.0353 - mean_squared_error: 0.0353\n",
      "Epoch 89/1000\n",
      "313/313 [==============================] - 0s 887us/step - loss: 0.0145 - mean_squared_error: 0.0145\n",
      "Epoch 90/1000\n",
      "313/313 [==============================] - 0s 909us/step - loss: 0.1871 - mean_squared_error: 0.1871\n",
      "Epoch 91/1000\n",
      "313/313 [==============================] - 0s 914us/step - loss: 0.0097 - mean_squared_error: 0.0097\n",
      "Epoch 92/1000\n",
      "313/313 [==============================] - 0s 861us/step - loss: 0.0130 - mean_squared_error: 0.0130\n",
      "Epoch 93/1000\n",
      "313/313 [==============================] - 0s 866us/step - loss: 0.1171 - mean_squared_error: 0.1171\n",
      "Epoch 94/1000\n",
      "313/313 [==============================] - 0s 885us/step - loss: 0.0186 - mean_squared_error: 0.0186\n",
      "Epoch 95/1000\n",
      "313/313 [==============================] - 0s 886us/step - loss: 0.0230 - mean_squared_error: 0.0230\n",
      "Epoch 96/1000\n",
      "313/313 [==============================] - 0s 874us/step - loss: 0.0094 - mean_squared_error: 0.0094\n",
      "Epoch 97/1000\n",
      "313/313 [==============================] - 0s 884us/step - loss: 0.0503 - mean_squared_error: 0.0503\n",
      "Epoch 98/1000\n",
      "313/313 [==============================] - 0s 867us/step - loss: 0.0366 - mean_squared_error: 0.0366\n",
      "Epoch 99/1000\n",
      "313/313 [==============================] - 0s 887us/step - loss: 0.0377 - mean_squared_error: 0.0377\n",
      "Epoch 100/1000\n",
      "313/313 [==============================] - 0s 887us/step - loss: 0.0667 - mean_squared_error: 0.0667\n",
      "Epoch 101/1000\n",
      "313/313 [==============================] - 0s 873us/step - loss: 0.0097 - mean_squared_error: 0.0097\n",
      "Epoch 102/1000\n",
      "313/313 [==============================] - 0s 886us/step - loss: 0.0480 - mean_squared_error: 0.0480\n",
      "Epoch 103/1000\n",
      "313/313 [==============================] - 0s 886us/step - loss: 0.0534 - mean_squared_error: 0.0534\n",
      "Epoch 104/1000\n",
      "313/313 [==============================] - 0s 867us/step - loss: 0.0034 - mean_squared_error: 0.0034\n",
      "Epoch 105/1000\n",
      "313/313 [==============================] - 0s 888us/step - loss: 0.0963 - mean_squared_error: 0.0963\n",
      "Epoch 106/1000\n",
      "313/313 [==============================] - 0s 887us/step - loss: 0.0057 - mean_squared_error: 0.0057\n",
      "Epoch 107/1000\n",
      "313/313 [==============================] - 0s 860us/step - loss: 0.0327 - mean_squared_error: 0.0327\n",
      "Epoch 108/1000\n",
      "313/313 [==============================] - 0s 869us/step - loss: 0.0077 - mean_squared_error: 0.0077\n",
      "Epoch 109/1000\n",
      "313/313 [==============================] - 0s 876us/step - loss: 0.0826 - mean_squared_error: 0.0826\n",
      "Epoch 110/1000\n",
      "313/313 [==============================] - 0s 866us/step - loss: 0.0287 - mean_squared_error: 0.0287\n",
      "Epoch 111/1000\n",
      "313/313 [==============================] - 0s 868us/step - loss: 0.0054 - mean_squared_error: 0.0054\n",
      "Epoch 112/1000\n",
      "313/313 [==============================] - 0s 870us/step - loss: 0.0179 - mean_squared_error: 0.0179\n",
      "Epoch 113/1000\n",
      "313/313 [==============================] - 0s 860us/step - loss: 0.0344 - mean_squared_error: 0.0344\n",
      "Epoch 114/1000\n",
      "313/313 [==============================] - 0s 888us/step - loss: 0.0439 - mean_squared_error: 0.0439\n",
      "Epoch 115/1000\n",
      "313/313 [==============================] - 0s 885us/step - loss: 0.0035 - mean_squared_error: 0.0035\n",
      "Epoch 116/1000\n",
      "313/313 [==============================] - 0s 863us/step - loss: 0.0107 - mean_squared_error: 0.0107\n",
      "Epoch 117/1000\n",
      "313/313 [==============================] - 0s 883us/step - loss: 0.0292 - mean_squared_error: 0.0292\n",
      "Epoch 118/1000\n",
      "313/313 [==============================] - 0s 856us/step - loss: 0.0114 - mean_squared_error: 0.0114\n",
      "Epoch 119/1000\n",
      "313/313 [==============================] - 0s 881us/step - loss: 0.0074 - mean_squared_error: 0.0074\n",
      "Epoch 120/1000\n",
      "313/313 [==============================] - 0s 921us/step - loss: 0.0082 - mean_squared_error: 0.0082\n",
      "Epoch 121/1000\n",
      "313/313 [==============================] - 0s 888us/step - loss: 0.0359 - mean_squared_error: 0.0359\n",
      "Epoch 122/1000\n",
      "313/313 [==============================] - 0s 884us/step - loss: 0.0196 - mean_squared_error: 0.0196\n",
      "Epoch 123/1000\n",
      "313/313 [==============================] - 0s 980us/step - loss: 0.0313 - mean_squared_error: 0.0313\n",
      "Epoch 124/1000\n",
      "313/313 [==============================] - 0s 893us/step - loss: 0.0311 - mean_squared_error: 0.0311\n",
      "Epoch 125/1000\n",
      "313/313 [==============================] - 0s 909us/step - loss: 0.0019 - mean_squared_error: 0.0019\n",
      "Epoch 126/1000\n",
      "313/313 [==============================] - 0s 945us/step - loss: 0.0076 - mean_squared_error: 0.0076\n",
      "Epoch 127/1000\n",
      "313/313 [==============================] - 0s 882us/step - loss: 0.0125 - mean_squared_error: 0.0125\n",
      "Epoch 128/1000\n",
      "313/313 [==============================] - 0s 851us/step - loss: 0.0437 - mean_squared_error: 0.0437\n",
      "Epoch 129/1000\n",
      "313/313 [==============================] - 0s 869us/step - loss: 0.0042 - mean_squared_error: 0.0042\n",
      "Epoch 130/1000\n",
      "313/313 [==============================] - 0s 876us/step - loss: 0.0408 - mean_squared_error: 0.0408\n",
      "Epoch 131/1000\n",
      "313/313 [==============================] - 0s 938us/step - loss: 0.0160 - mean_squared_error: 0.0160\n",
      "Epoch 132/1000\n",
      "313/313 [==============================] - 0s 904us/step - loss: 0.0032 - mean_squared_error: 0.0032\n",
      "Epoch 133/1000\n",
      "313/313 [==============================] - 0s 915us/step - loss: 0.0059 - mean_squared_error: 0.0059\n",
      "Epoch 134/1000\n",
      "313/313 [==============================] - 0s 879us/step - loss: 0.0156 - mean_squared_error: 0.0156\n",
      "Epoch 135/1000\n",
      "313/313 [==============================] - 0s 891us/step - loss: 0.0158 - mean_squared_error: 0.0158\n",
      "Epoch 136/1000\n",
      "313/313 [==============================] - 0s 891us/step - loss: 0.0087 - mean_squared_error: 0.0087\n",
      "Epoch 137/1000\n",
      "313/313 [==============================] - 0s 894us/step - loss: 0.0122 - mean_squared_error: 0.0122\n",
      "Epoch 138/1000\n",
      "313/313 [==============================] - 0s 889us/step - loss: 0.0059 - mean_squared_error: 0.0059\n",
      "Epoch 139/1000\n",
      "313/313 [==============================] - 0s 919us/step - loss: 0.0136 - mean_squared_error: 0.0136\n",
      "Epoch 140/1000\n",
      "313/313 [==============================] - 0s 877us/step - loss: 0.0021 - mean_squared_error: 0.0021\n",
      "Epoch 141/1000\n",
      "313/313 [==============================] - 0s 905us/step - loss: 0.0123 - mean_squared_error: 0.0123\n",
      "Epoch 142/1000\n",
      "313/313 [==============================] - 0s 891us/step - loss: 0.0147 - mean_squared_error: 0.0147\n",
      "Epoch 143/1000\n",
      "313/313 [==============================] - 0s 875us/step - loss: 0.0084 - mean_squared_error: 0.0084\n",
      "Epoch 144/1000\n",
      "313/313 [==============================] - 0s 876us/step - loss: 0.0121 - mean_squared_error: 0.0121\n",
      "Epoch 145/1000\n",
      "313/313 [==============================] - 0s 874us/step - loss: 0.0637 - mean_squared_error: 0.0637\n",
      "Epoch 146/1000\n",
      "313/313 [==============================] - 0s 911us/step - loss: 0.0036 - mean_squared_error: 0.0036\n",
      "Epoch 147/1000\n",
      "313/313 [==============================] - 0s 899us/step - loss: 0.0021 - mean_squared_error: 0.0021\n",
      "Epoch 148/1000\n",
      "273/313 [=========================>....] - ETA: 0s - loss: 7.8997e-04 - mean_squared_error: 7.8997e-04"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/gabriel/Projects Github/BarenNet/src/notebooks/BarenNet_Laminar.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/gabriel/Projects%20Github/BarenNet/src/notebooks/BarenNet_Laminar.ipynb#ch0000007?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(xtrain, ytrain, epochs \u001b[39m=\u001b[39;49m \u001b[39m1000\u001b[39;49m, verbose \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/training.py:1414\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1412\u001b[0m logs \u001b[39m=\u001b[39m tmp_logs  \u001b[39m# No error, now safe to assign to logs.\u001b[39;00m\n\u001b[1;32m   1413\u001b[0m end_step \u001b[39m=\u001b[39m step \u001b[39m+\u001b[39m data_handler\u001b[39m.\u001b[39mstep_increment\n\u001b[0;32m-> 1414\u001b[0m callbacks\u001b[39m.\u001b[39;49mon_train_batch_end(end_step, logs)\n\u001b[1;32m   1415\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_training:\n\u001b[1;32m   1416\u001b[0m   \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/callbacks.py:438\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[39m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \n\u001b[1;32m    433\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[39m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[39m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[0;32m--> 438\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook(ModeKeys\u001b[39m.\u001b[39;49mTRAIN, \u001b[39m'\u001b[39;49m\u001b[39mend\u001b[39;49m\u001b[39m'\u001b[39;49m, batch, logs\u001b[39m=\u001b[39;49mlogs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/callbacks.py:297\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    295\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[1;32m    296\u001b[0m \u001b[39melif\u001b[39;00m hook \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mend\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 297\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_end_hook(mode, batch, logs)\n\u001b[1;32m    298\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    300\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mUnrecognized hook: \u001b[39m\u001b[39m{\u001b[39;00mhook\u001b[39m}\u001b[39;00m\u001b[39m. Expected values are [\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbegin\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/callbacks.py:318\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    315\u001b[0m   batch_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_start_time\n\u001b[1;32m    316\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times\u001b[39m.\u001b[39mappend(batch_time)\n\u001b[0;32m--> 318\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook_helper(hook_name, batch, logs)\n\u001b[1;32m    320\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_batches_for_timing_check:\n\u001b[1;32m    321\u001b[0m   end_hook_name \u001b[39m=\u001b[39m hook_name\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/callbacks.py:358\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    355\u001b[0m   hook \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(callback, hook_name)\n\u001b[1;32m    356\u001b[0m   hook(batch, logs)\n\u001b[0;32m--> 358\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_timing:\n\u001b[1;32m    359\u001b[0m   \u001b[39mif\u001b[39;00m hook_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hook_times:\n\u001b[1;32m    360\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hook_times[hook_name] \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(xtrain, ytrain, epochs = 1000, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exponents multiplicating inside the function for y+: [-0.9994309]\n",
      "Exponents multiplicating outside the function: [0.99976444]\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_nonsimilar):\n",
    "    similarity_exponents = model.get_layer('similarity_layer_' + str(i+1)).weights[0][0].numpy()\n",
    "    print('Exponents multiplicating inside the function for ' + nonsimilar_keys[i] + ': ' + str(similarity_exponents))\n",
    "\n",
    "multiplication_exponents = model.get_layer('multiplication_layer').weights[0][0].numpy()\n",
    "print('Exponents multiplicating outside the function: ' + str(multiplication_exponents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
